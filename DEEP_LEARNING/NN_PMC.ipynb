{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN PMC avec Keras\n",
    "\n",
    "\n",
    "#### Yulia Kalugina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargememnt des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L’objectif de ces exercices est de pratiquer le perceptron simple et le perceptron\n",
    "multicouche en utilisant la bibliothèque Keras.\n",
    "\n",
    "- Soit l’ensemble des données Segmentation (disponible aussi sur Lea) qui\n",
    "comprend 2310 observations d’images décrites par 19 variables (caractéristiques)\n",
    "et leurs variable cibles (classe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Téléchargez le contenu de la base de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_centroid_col</th>\n",
       "      <th>region_centroid_row</th>\n",
       "      <th>region_pixel_count</th>\n",
       "      <th>short_line_density_5</th>\n",
       "      <th>short_line_density_2</th>\n",
       "      <th>vedge_mean</th>\n",
       "      <th>vegde_sd</th>\n",
       "      <th>hedge_mean</th>\n",
       "      <th>hedge_sd</th>\n",
       "      <th>intensity_mean</th>\n",
       "      <th>rawred_mean</th>\n",
       "      <th>rawblue_mean</th>\n",
       "      <th>rawgreen_mean</th>\n",
       "      <th>exred_mean</th>\n",
       "      <th>exblue_mean</th>\n",
       "      <th>exgreen_mean</th>\n",
       "      <th>value_mean</th>\n",
       "      <th>saturation_mean</th>\n",
       "      <th>hue_mean</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>218</td>\n",
       "      <td>178</td>\n",
       "      <td>9</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.54</td>\n",
       "      <td>59.63</td>\n",
       "      <td>52.44</td>\n",
       "      <td>75.22</td>\n",
       "      <td>51.22</td>\n",
       "      <td>-21.56</td>\n",
       "      <td>46.78</td>\n",
       "      <td>-25.22</td>\n",
       "      <td>75.22</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-2.04</td>\n",
       "      <td>path</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>113</td>\n",
       "      <td>130</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-2.67</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>2.56</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-2.12</td>\n",
       "      <td>foliage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>202</td>\n",
       "      <td>41</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.03</td>\n",
       "      <td>123.04</td>\n",
       "      <td>111.89</td>\n",
       "      <td>139.78</td>\n",
       "      <td>117.44</td>\n",
       "      <td>-33.44</td>\n",
       "      <td>50.22</td>\n",
       "      <td>-16.78</td>\n",
       "      <td>139.78</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-2.30</td>\n",
       "      <td>sky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>173</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.78</td>\n",
       "      <td>9.00</td>\n",
       "      <td>6.75</td>\n",
       "      <td>43.59</td>\n",
       "      <td>39.56</td>\n",
       "      <td>52.89</td>\n",
       "      <td>38.33</td>\n",
       "      <td>-12.11</td>\n",
       "      <td>27.89</td>\n",
       "      <td>-15.78</td>\n",
       "      <td>52.89</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>path</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>197</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.52</td>\n",
       "      <td>2.61</td>\n",
       "      <td>1.93</td>\n",
       "      <td>49.59</td>\n",
       "      <td>44.22</td>\n",
       "      <td>61.56</td>\n",
       "      <td>43.00</td>\n",
       "      <td>-16.11</td>\n",
       "      <td>35.89</td>\n",
       "      <td>-19.78</td>\n",
       "      <td>61.56</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>path</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305</th>\n",
       "      <td>30</td>\n",
       "      <td>102</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.80</td>\n",
       "      <td>20.26</td>\n",
       "      <td>20.33</td>\n",
       "      <td>25.00</td>\n",
       "      <td>15.44</td>\n",
       "      <td>0.22</td>\n",
       "      <td>14.22</td>\n",
       "      <td>-14.44</td>\n",
       "      <td>25.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-1.56</td>\n",
       "      <td>brickface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>143</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.14</td>\n",
       "      <td>127.63</td>\n",
       "      <td>117.67</td>\n",
       "      <td>141.67</td>\n",
       "      <td>123.56</td>\n",
       "      <td>-29.89</td>\n",
       "      <td>42.11</td>\n",
       "      <td>-12.22</td>\n",
       "      <td>141.67</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-2.35</td>\n",
       "      <td>sky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2307</th>\n",
       "      <td>80</td>\n",
       "      <td>72</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.44</td>\n",
       "      <td>1.17</td>\n",
       "      <td>59.00</td>\n",
       "      <td>51.33</td>\n",
       "      <td>74.44</td>\n",
       "      <td>51.22</td>\n",
       "      <td>-23.00</td>\n",
       "      <td>46.33</td>\n",
       "      <td>-23.33</td>\n",
       "      <td>74.44</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-2.09</td>\n",
       "      <td>cement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>98</td>\n",
       "      <td>133</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-2.89</td>\n",
       "      <td>5.44</td>\n",
       "      <td>-2.56</td>\n",
       "      <td>2.78</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-2.12</td>\n",
       "      <td>foliage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309</th>\n",
       "      <td>19</td>\n",
       "      <td>147</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4.15</td>\n",
       "      <td>3.89</td>\n",
       "      <td>6.67</td>\n",
       "      <td>1.89</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>7.56</td>\n",
       "      <td>-6.78</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>-1.48</td>\n",
       "      <td>brickface</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2310 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      region_centroid_col  region_centroid_row  region_pixel_count  \\\n",
       "0                     218                  178                   9   \n",
       "1                     113                  130                   9   \n",
       "2                     202                   41                   9   \n",
       "3                      32                  173                   9   \n",
       "4                      61                  197                   9   \n",
       "...                   ...                  ...                 ...   \n",
       "2305                   30                  102                   9   \n",
       "2306                  143                   24                   9   \n",
       "2307                   80                   72                   9   \n",
       "2308                   98                  133                   9   \n",
       "2309                   19                  147                   9   \n",
       "\n",
       "      short_line_density_5  short_line_density_2  vedge_mean  vegde_sd  \\\n",
       "0                     0.11                   0.0        0.83      0.55   \n",
       "1                     0.00                   0.0        0.28      0.25   \n",
       "2                     0.00                   0.0        0.94      0.77   \n",
       "3                     0.00                   0.0        1.72      1.78   \n",
       "4                     0.00                   0.0        1.44      1.52   \n",
       "...                    ...                   ...         ...       ...   \n",
       "2305                  0.00                   0.0        1.22      0.12   \n",
       "2306                  0.00                   0.0        1.28      0.91   \n",
       "2307                  0.00                   0.0        1.22      1.00   \n",
       "2308                  0.00                   0.0        0.56      0.17   \n",
       "2309                  0.00                   0.0        0.22      0.07   \n",
       "\n",
       "      hedge_mean  hedge_sd  intensity_mean  rawred_mean  rawblue_mean  \\\n",
       "0           1.11      0.54           59.63        52.44         75.22   \n",
       "1           0.33      0.37            0.89         0.00          2.56   \n",
       "2           1.11      1.03          123.04       111.89        139.78   \n",
       "3           9.00      6.75           43.59        39.56         52.89   \n",
       "4           2.61      1.93           49.59        44.22         61.56   \n",
       "...          ...       ...             ...          ...           ...   \n",
       "2305        1.33      0.80           20.26        20.33         25.00   \n",
       "2306        0.89      1.14          127.63       117.67        141.67   \n",
       "2307        1.44      1.17           59.00        51.33         74.44   \n",
       "2308        0.39      0.33            0.96         0.00          2.78   \n",
       "2309        0.50      0.08            4.15         3.89          6.67   \n",
       "\n",
       "      rawgreen_mean  exred_mean  exblue_mean  exgreen_mean  value_mean  \\\n",
       "0             51.22      -21.56        46.78        -25.22       75.22   \n",
       "1              0.11       -2.67         5.00         -2.33        2.56   \n",
       "2            117.44      -33.44        50.22        -16.78      139.78   \n",
       "3             38.33      -12.11        27.89        -15.78       52.89   \n",
       "4             43.00      -16.11        35.89        -19.78       61.56   \n",
       "...             ...         ...          ...           ...         ...   \n",
       "2305          15.44        0.22        14.22        -14.44       25.00   \n",
       "2306         123.56      -29.89        42.11        -12.22      141.67   \n",
       "2307          51.22      -23.00        46.33        -23.33       74.44   \n",
       "2308           0.11       -2.89         5.44         -2.56        2.78   \n",
       "2309           1.89       -0.78         7.56         -6.78        7.00   \n",
       "\n",
       "      saturation_mean  hue_mean     classe  \n",
       "0                0.32     -2.04       path  \n",
       "1                1.00     -2.12    foliage  \n",
       "2                0.20     -2.30        sky  \n",
       "3                0.27     -2.00       path  \n",
       "4                0.30     -2.02       path  \n",
       "...               ...       ...        ...  \n",
       "2305             0.38     -1.56  brickface  \n",
       "2306             0.17     -2.35        sky  \n",
       "2307             0.31     -2.09     cement  \n",
       "2308             1.00     -2.12    foliage  \n",
       "2309             0.71     -1.48  brickface  \n",
       "\n",
       "[2310 rows x 20 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('segmentation.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Procédez à une standardisation des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2310 entries, 0 to 2309\n",
      "Data columns (total 20 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   region_centroid_col   2310 non-null   int64  \n",
      " 1   region_centroid_row   2310 non-null   int64  \n",
      " 2   region_pixel_count    2310 non-null   int64  \n",
      " 3   short_line_density_5  2310 non-null   float64\n",
      " 4   short_line_density_2  2310 non-null   float64\n",
      " 5   vedge_mean            2310 non-null   float64\n",
      " 6   vegde_sd              2310 non-null   float64\n",
      " 7   hedge_mean            2310 non-null   float64\n",
      " 8   hedge_sd              2310 non-null   float64\n",
      " 9   intensity_mean        2310 non-null   float64\n",
      " 10  rawred_mean           2310 non-null   float64\n",
      " 11  rawblue_mean          2310 non-null   float64\n",
      " 12  rawgreen_mean         2310 non-null   float64\n",
      " 13  exred_mean            2310 non-null   float64\n",
      " 14  exblue_mean           2310 non-null   float64\n",
      " 15  exgreen_mean          2310 non-null   float64\n",
      " 16  value_mean            2310 non-null   float64\n",
      " 17  saturation_mean       2310 non-null   float64\n",
      " 18  hue_mean              2310 non-null   float64\n",
      " 19  classe                2310 non-null   object \n",
      "dtypes: float64(16), int64(3), object(1)\n",
      "memory usage: 361.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.copy()\n",
    "df1.drop(columns=['classe'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "  \n",
    "# transform data\n",
    "df_std = scaler.fit_transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.2761887 ,  0.94973634,  0.        , ...,  0.70102145,\n",
       "        -0.46877222, -0.43831817],\n",
       "       [-0.16333606,  0.11453842,  0.        , ..., -0.99219481,\n",
       "         2.51046151, -0.49010061],\n",
       "       [ 1.05683255, -1.43405774,  0.        , ...,  2.20548115,\n",
       "        -0.99451935, -0.6066111 ],\n",
       "       ...,\n",
       "       [-0.61575812, -0.89465908,  0.        , ...,  0.68284489,\n",
       "        -0.51258448, -0.4706822 ],\n",
       "       [-0.36898245,  0.16673829,  0.        , ..., -0.98706809,\n",
       "         2.51046151, -0.49010061],\n",
       "       [-1.45205346,  0.41033768,  0.        , ..., -0.88872825,\n",
       "         1.23990594, -0.0758411 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Déterminez les différentes classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les classes:\n",
      "\n",
      "path\n",
      "cement\n",
      "foliage\n",
      "sky\n",
      "window\n",
      "grass\n",
      "brickface\n"
     ]
    }
   ],
   "source": [
    "print('Les classes:\\n')\n",
    "dd=(list(df['classe'].value_counts().index))\n",
    "class_names=[]\n",
    "for i in (dd):\n",
    "    print(i)\n",
    "    class_names.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_centroid_col</th>\n",
       "      <th>region_centroid_row</th>\n",
       "      <th>region_pixel_count</th>\n",
       "      <th>short_line_density_5</th>\n",
       "      <th>short_line_density_2</th>\n",
       "      <th>vedge_mean</th>\n",
       "      <th>vegde_sd</th>\n",
       "      <th>hedge_mean</th>\n",
       "      <th>hedge_sd</th>\n",
       "      <th>intensity_mean</th>\n",
       "      <th>rawred_mean</th>\n",
       "      <th>rawblue_mean</th>\n",
       "      <th>rawgreen_mean</th>\n",
       "      <th>exred_mean</th>\n",
       "      <th>exblue_mean</th>\n",
       "      <th>exgreen_mean</th>\n",
       "      <th>value_mean</th>\n",
       "      <th>saturation_mean</th>\n",
       "      <th>hue_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.276189</td>\n",
       "      <td>0.949736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.410668</td>\n",
       "      <td>-0.194552</td>\n",
       "      <td>-0.394306</td>\n",
       "      <td>-0.115068</td>\n",
       "      <td>-0.364251</td>\n",
       "      <td>-0.131018</td>\n",
       "      <td>0.591553</td>\n",
       "      <td>0.560068</td>\n",
       "      <td>0.713087</td>\n",
       "      <td>0.469629</td>\n",
       "      <td>-0.765840</td>\n",
       "      <td>1.296593</td>\n",
       "      <td>-1.428854</td>\n",
       "      <td>0.701021</td>\n",
       "      <td>-0.468772</td>\n",
       "      <td>-0.438318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.163336</td>\n",
       "      <td>0.114538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.357047</td>\n",
       "      <td>-0.194552</td>\n",
       "      <td>-0.598129</td>\n",
       "      <td>-0.121759</td>\n",
       "      <td>-0.580366</td>\n",
       "      <td>-0.133910</td>\n",
       "      <td>-0.947427</td>\n",
       "      <td>-0.936970</td>\n",
       "      <td>-0.956566</td>\n",
       "      <td>-0.936155</td>\n",
       "      <td>0.865280</td>\n",
       "      <td>-0.838569</td>\n",
       "      <td>0.553118</td>\n",
       "      <td>-0.992195</td>\n",
       "      <td>2.510462</td>\n",
       "      <td>-0.490101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.056833</td>\n",
       "      <td>-1.434058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.357047</td>\n",
       "      <td>-0.194552</td>\n",
       "      <td>-0.353541</td>\n",
       "      <td>-0.110161</td>\n",
       "      <td>-0.364251</td>\n",
       "      <td>-0.122685</td>\n",
       "      <td>2.252887</td>\n",
       "      <td>2.257225</td>\n",
       "      <td>2.196611</td>\n",
       "      <td>2.291015</td>\n",
       "      <td>-1.791659</td>\n",
       "      <td>1.472394</td>\n",
       "      <td>-0.698062</td>\n",
       "      <td>2.205481</td>\n",
       "      <td>-0.994519</td>\n",
       "      <td>-0.606611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.273827</td>\n",
       "      <td>0.862737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.357047</td>\n",
       "      <td>-0.194552</td>\n",
       "      <td>-0.064484</td>\n",
       "      <td>-0.087635</td>\n",
       "      <td>1.821843</td>\n",
       "      <td>-0.025404</td>\n",
       "      <td>0.171307</td>\n",
       "      <td>0.192374</td>\n",
       "      <td>0.199966</td>\n",
       "      <td>0.115089</td>\n",
       "      <td>0.050152</td>\n",
       "      <td>0.331222</td>\n",
       "      <td>-0.611475</td>\n",
       "      <td>0.180659</td>\n",
       "      <td>-0.687834</td>\n",
       "      <td>-0.412427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.876244</td>\n",
       "      <td>1.280336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.357047</td>\n",
       "      <td>-0.194552</td>\n",
       "      <td>-0.168248</td>\n",
       "      <td>-0.093434</td>\n",
       "      <td>0.051357</td>\n",
       "      <td>-0.107378</td>\n",
       "      <td>0.328506</td>\n",
       "      <td>0.325406</td>\n",
       "      <td>0.399194</td>\n",
       "      <td>0.243538</td>\n",
       "      <td>-0.295242</td>\n",
       "      <td>0.740061</td>\n",
       "      <td>-0.957822</td>\n",
       "      <td>0.382699</td>\n",
       "      <td>-0.556397</td>\n",
       "      <td>-0.425373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305</th>\n",
       "      <td>-1.301246</td>\n",
       "      <td>-0.372660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.357047</td>\n",
       "      <td>-0.194552</td>\n",
       "      <td>-0.249777</td>\n",
       "      <td>-0.124658</td>\n",
       "      <td>-0.303295</td>\n",
       "      <td>-0.126597</td>\n",
       "      <td>-0.439936</td>\n",
       "      <td>-0.356597</td>\n",
       "      <td>-0.440918</td>\n",
       "      <td>-0.514502</td>\n",
       "      <td>1.114827</td>\n",
       "      <td>-0.367382</td>\n",
       "      <td>-0.495449</td>\n",
       "      <td>-0.469269</td>\n",
       "      <td>-0.205899</td>\n",
       "      <td>-0.127624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>0.247957</td>\n",
       "      <td>-1.729857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.357047</td>\n",
       "      <td>-0.194552</td>\n",
       "      <td>-0.227542</td>\n",
       "      <td>-0.107039</td>\n",
       "      <td>-0.425206</td>\n",
       "      <td>-0.120814</td>\n",
       "      <td>2.373144</td>\n",
       "      <td>2.422230</td>\n",
       "      <td>2.240041</td>\n",
       "      <td>2.459346</td>\n",
       "      <td>-1.485122</td>\n",
       "      <td>1.057933</td>\n",
       "      <td>-0.303226</td>\n",
       "      <td>2.249524</td>\n",
       "      <td>-1.125956</td>\n",
       "      <td>-0.638975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2307</th>\n",
       "      <td>-0.615758</td>\n",
       "      <td>-0.894659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.357047</td>\n",
       "      <td>-0.194552</td>\n",
       "      <td>-0.249777</td>\n",
       "      <td>-0.105031</td>\n",
       "      <td>-0.272817</td>\n",
       "      <td>-0.120304</td>\n",
       "      <td>0.575047</td>\n",
       "      <td>0.528380</td>\n",
       "      <td>0.695164</td>\n",
       "      <td>0.469629</td>\n",
       "      <td>-0.890182</td>\n",
       "      <td>1.273596</td>\n",
       "      <td>-1.265205</td>\n",
       "      <td>0.682845</td>\n",
       "      <td>-0.512584</td>\n",
       "      <td>-0.470682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>-0.368982</td>\n",
       "      <td>0.166738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.357047</td>\n",
       "      <td>-0.194552</td>\n",
       "      <td>-0.494364</td>\n",
       "      <td>-0.123543</td>\n",
       "      <td>-0.563742</td>\n",
       "      <td>-0.134590</td>\n",
       "      <td>-0.945593</td>\n",
       "      <td>-0.936970</td>\n",
       "      <td>-0.951511</td>\n",
       "      <td>-0.936155</td>\n",
       "      <td>0.846283</td>\n",
       "      <td>-0.816083</td>\n",
       "      <td>0.533203</td>\n",
       "      <td>-0.987068</td>\n",
       "      <td>2.510462</td>\n",
       "      <td>-0.490101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309</th>\n",
       "      <td>-1.452053</td>\n",
       "      <td>0.410338</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.357047</td>\n",
       "      <td>-0.194552</td>\n",
       "      <td>-0.620364</td>\n",
       "      <td>-0.125773</td>\n",
       "      <td>-0.533264</td>\n",
       "      <td>-0.138842</td>\n",
       "      <td>-0.862016</td>\n",
       "      <td>-0.825920</td>\n",
       "      <td>-0.862123</td>\n",
       "      <td>-0.887196</td>\n",
       "      <td>1.028478</td>\n",
       "      <td>-0.707741</td>\n",
       "      <td>0.167806</td>\n",
       "      <td>-0.888728</td>\n",
       "      <td>1.239906</td>\n",
       "      <td>-0.075841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2310 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      region_centroid_col  region_centroid_row  region_pixel_count  \\\n",
       "0                1.276189             0.949736                 0.0   \n",
       "1               -0.163336             0.114538                 0.0   \n",
       "2                1.056833            -1.434058                 0.0   \n",
       "3               -1.273827             0.862737                 0.0   \n",
       "4               -0.876244             1.280336                 0.0   \n",
       "...                   ...                  ...                 ...   \n",
       "2305            -1.301246            -0.372660                 0.0   \n",
       "2306             0.247957            -1.729857                 0.0   \n",
       "2307            -0.615758            -0.894659                 0.0   \n",
       "2308            -0.368982             0.166738                 0.0   \n",
       "2309            -1.452053             0.410338                 0.0   \n",
       "\n",
       "      short_line_density_5  short_line_density_2  vedge_mean  vegde_sd  \\\n",
       "0                 2.410668             -0.194552   -0.394306 -0.115068   \n",
       "1                -0.357047             -0.194552   -0.598129 -0.121759   \n",
       "2                -0.357047             -0.194552   -0.353541 -0.110161   \n",
       "3                -0.357047             -0.194552   -0.064484 -0.087635   \n",
       "4                -0.357047             -0.194552   -0.168248 -0.093434   \n",
       "...                    ...                   ...         ...       ...   \n",
       "2305             -0.357047             -0.194552   -0.249777 -0.124658   \n",
       "2306             -0.357047             -0.194552   -0.227542 -0.107039   \n",
       "2307             -0.357047             -0.194552   -0.249777 -0.105031   \n",
       "2308             -0.357047             -0.194552   -0.494364 -0.123543   \n",
       "2309             -0.357047             -0.194552   -0.620364 -0.125773   \n",
       "\n",
       "      hedge_mean  hedge_sd  intensity_mean  rawred_mean  rawblue_mean  \\\n",
       "0      -0.364251 -0.131018        0.591553     0.560068      0.713087   \n",
       "1      -0.580366 -0.133910       -0.947427    -0.936970     -0.956566   \n",
       "2      -0.364251 -0.122685        2.252887     2.257225      2.196611   \n",
       "3       1.821843 -0.025404        0.171307     0.192374      0.199966   \n",
       "4       0.051357 -0.107378        0.328506     0.325406      0.399194   \n",
       "...          ...       ...             ...          ...           ...   \n",
       "2305   -0.303295 -0.126597       -0.439936    -0.356597     -0.440918   \n",
       "2306   -0.425206 -0.120814        2.373144     2.422230      2.240041   \n",
       "2307   -0.272817 -0.120304        0.575047     0.528380      0.695164   \n",
       "2308   -0.563742 -0.134590       -0.945593    -0.936970     -0.951511   \n",
       "2309   -0.533264 -0.138842       -0.862016    -0.825920     -0.862123   \n",
       "\n",
       "      rawgreen_mean  exred_mean  exblue_mean  exgreen_mean  value_mean  \\\n",
       "0          0.469629   -0.765840     1.296593     -1.428854    0.701021   \n",
       "1         -0.936155    0.865280    -0.838569      0.553118   -0.992195   \n",
       "2          2.291015   -1.791659     1.472394     -0.698062    2.205481   \n",
       "3          0.115089    0.050152     0.331222     -0.611475    0.180659   \n",
       "4          0.243538   -0.295242     0.740061     -0.957822    0.382699   \n",
       "...             ...         ...          ...           ...         ...   \n",
       "2305      -0.514502    1.114827    -0.367382     -0.495449   -0.469269   \n",
       "2306       2.459346   -1.485122     1.057933     -0.303226    2.249524   \n",
       "2307       0.469629   -0.890182     1.273596     -1.265205    0.682845   \n",
       "2308      -0.936155    0.846283    -0.816083      0.533203   -0.987068   \n",
       "2309      -0.887196    1.028478    -0.707741      0.167806   -0.888728   \n",
       "\n",
       "      saturation_mean  hue_mean  \n",
       "0           -0.468772 -0.438318  \n",
       "1            2.510462 -0.490101  \n",
       "2           -0.994519 -0.606611  \n",
       "3           -0.687834 -0.412427  \n",
       "4           -0.556397 -0.425373  \n",
       "...               ...       ...  \n",
       "2305        -0.205899 -0.127624  \n",
       "2306        -1.125956 -0.638975  \n",
       "2307        -0.512584 -0.470682  \n",
       "2308         2.510462 -0.490101  \n",
       "2309         1.239906 -0.075841  \n",
       "\n",
       "[2310 rows x 19 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.DataFrame(df_std, columns = df1.columns)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>brickface</th>\n",
       "      <th>cement</th>\n",
       "      <th>foliage</th>\n",
       "      <th>grass</th>\n",
       "      <th>path</th>\n",
       "      <th>sky</th>\n",
       "      <th>window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2306</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2307</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2309</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2310 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     brickface cement foliage grass path  sky window\n",
       "0          0.0    0.0     0.0   0.0  1.0  0.0    0.0\n",
       "1          0.0    0.0     1.0   0.0  0.0  0.0    0.0\n",
       "2          0.0    0.0     0.0   0.0  0.0  1.0    0.0\n",
       "3          0.0    0.0     0.0   0.0  1.0  0.0    0.0\n",
       "4          0.0    0.0     0.0   0.0  1.0  0.0    0.0\n",
       "...        ...    ...     ...   ...  ...  ...    ...\n",
       "2305       1.0    0.0     0.0   0.0  0.0  0.0    0.0\n",
       "2306       0.0    0.0     0.0   0.0  0.0  1.0    0.0\n",
       "2307       0.0    1.0     0.0   0.0  0.0  0.0    0.0\n",
       "2308       0.0    0.0     1.0   0.0  0.0  0.0    0.0\n",
       "2309       1.0    0.0     0.0   0.0  0.0  0.0    0.0\n",
       "\n",
       "[2310 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "#encodage de classes\n",
    "enc =OneHotEncoder()\n",
    "res= enc.fit_transform(df[[\"classe\"]])\n",
    "labels=pd.DataFrame(res.toarray(), columns=enc.categories_)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['brickface', 'cement', 'foliage', 'grass', 'path', 'sky', 'window'],\n",
       "       dtype=object)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the dataframes of variables and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tot=pd.concat([df2,labels],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Considérez une partition de 70% pour l’entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "#Utilisation de stratification\n",
    "train, test = train_test_split(df_tot, test_size=0.3,  stratify=df_tot[enc.categories_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Vérifiez la taille de l’échantillon d’entrainement et de test par classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install prettytable\n",
    "from prettytable import PrettyTable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN \n",
      " +--------------+------+-----+\n",
      "|   classe     | 0.0  | 1.0 |\n",
      "+--------------+------+-----+\n",
      "|  brickface   | 1386 | 231 |\n",
      "|    cement    | 1386 | 231 |\n",
      "|   foliage    | 1386 | 231 |\n",
      "|    grass     | 1386 | 231 |\n",
      "|     path     | 1386 | 231 |\n",
      "|     sky      | 1386 | 231 |\n",
      "|    window    | 1386 | 231 |\n",
      "+--------------+------+-----+\n",
      "TEST \n",
      " +--------------+-----+-----+\n",
      "|   classe     | 0.0 | 1.0 |\n",
      "+--------------+-----+-----+\n",
      "|  brickface   | 594 |  99 |\n",
      "|    cement    | 594 |  99 |\n",
      "|   foliage    | 594 |  99 |\n",
      "|    grass     | 594 |  99 |\n",
      "|     path     | 594 |  99 |\n",
      "|     sky      | 594 |  99 |\n",
      "|    window    | 594 |  99 |\n",
      "+--------------+-----+-----+\n"
     ]
    }
   ],
   "source": [
    "# Specify the Column Names while initializing the Table \n",
    "train_classe = PrettyTable(['  classe    ',train[columns].value_counts().index[0],train[columns].value_counts().index[1]]) \n",
    "test_classe = PrettyTable(['  classe    ',train[columns].value_counts().index[0],train[columns].value_counts().index[1]]) \n",
    "    \n",
    "# Add rows \n",
    "for columns in labels.columns:\n",
    "    train_classe.add_row([str(columns)[2:-3],train[columns].value_counts()[0],train[columns].value_counts()[1]])\n",
    "    test_classe.add_row([str(columns)[2:-3],test[columns].value_counts()[0],test[columns].value_counts()[1]])\n",
    "\n",
    "print('TRAIN \\n',train_classe)\n",
    "print('TEST \\n',test_classe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On voit que les donnees sont bien stratifiees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([ 'region_centroid_col',  'region_centroid_row',   'region_pixel_count',\n",
       "       'short_line_density_5', 'short_line_density_2',           'vedge_mean',\n",
       "                   'vegde_sd',           'hedge_mean',             'hedge_sd',\n",
       "             'intensity_mean',          'rawred_mean',         'rawblue_mean',\n",
       "              'rawgreen_mean',           'exred_mean',          'exblue_mean',\n",
       "               'exgreen_mean',           'value_mean',      'saturation_mean',\n",
       "                   'hue_mean',         ('brickface',),            ('cement',),\n",
       "                 ('foliage',),             ('grass',),              ('path',),\n",
       "                     ('sky',),            ('window',)],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train[['region_centroid_col', 'region_centroid_row', 'region_pixel_count',\n",
    "       'short_line_density_5', 'short_line_density_2', 'vedge_mean',\n",
    "       'vegde_sd', 'hedge_mean', 'hedge_sd', 'intensity_mean', 'rawred_mean',\n",
    "       'rawblue_mean', 'rawgreen_mean', 'exred_mean', 'exblue_mean',\n",
    "       'exgreen_mean', 'value_mean', 'saturation_mean', 'hue_mean']]\n",
    "y_train=train[enc.categories_]\n",
    "X_test=test[['region_centroid_col', 'region_centroid_row', 'region_pixel_count',\n",
    "       'short_line_density_5', 'short_line_density_2', 'vedge_mean',\n",
    "       'vegde_sd', 'hedge_mean', 'hedge_sd', 'intensity_mean', 'rawred_mean',\n",
    "       'rawblue_mean', 'rawgreen_mean', 'exred_mean', 'exblue_mean',\n",
    "       'exgreen_mean', 'value_mean', 'saturation_mean', 'hue_mean']]\n",
    "y_test=test[enc.categories_] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Développez un perceptron simple et une architecture séquentielle (activation=’softmax’, optimizer=’adam’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers : 1\n",
      "Training\n",
      "Epoch 1/30\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 1.7353 - accuracy: 0.2907 - val_loss: 1.5794 - val_accuracy: 0.3550\n",
      "Epoch 2/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 1.5209 - accuracy: 0.3939 - val_loss: 1.4056 - val_accuracy: 0.4719\n",
      "Epoch 3/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 1.3624 - accuracy: 0.4892 - val_loss: 1.2675 - val_accuracy: 0.5801\n",
      "Epoch 4/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 1.2347 - accuracy: 0.5850 - val_loss: 1.1558 - val_accuracy: 0.6436\n",
      "Epoch 5/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 1.1301 - accuracy: 0.6494 - val_loss: 1.0633 - val_accuracy: 0.6797\n",
      "Epoch 6/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 1.0435 - accuracy: 0.6784 - val_loss: 0.9874 - val_accuracy: 0.6912\n",
      "Epoch 7/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.9722 - accuracy: 0.6964 - val_loss: 0.9224 - val_accuracy: 0.7114\n",
      "Epoch 8/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.9119 - accuracy: 0.7180 - val_loss: 0.8690 - val_accuracy: 0.7388\n",
      "Epoch 9/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.8609 - accuracy: 0.7458 - val_loss: 0.8224 - val_accuracy: 0.7778\n",
      "Epoch 10/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.8160 - accuracy: 0.7687 - val_loss: 0.7826 - val_accuracy: 0.8124\n",
      "Epoch 11/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.7774 - accuracy: 0.7984 - val_loss: 0.7469 - val_accuracy: 0.8254\n",
      "Epoch 12/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.7435 - accuracy: 0.8139 - val_loss: 0.7158 - val_accuracy: 0.8297\n",
      "Epoch 13/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.7134 - accuracy: 0.8256 - val_loss: 0.6883 - val_accuracy: 0.8413\n",
      "Epoch 14/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.6865 - accuracy: 0.8330 - val_loss: 0.6629 - val_accuracy: 0.8442\n",
      "Epoch 15/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.6622 - accuracy: 0.8411 - val_loss: 0.6407 - val_accuracy: 0.8499\n",
      "Epoch 16/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.6402 - accuracy: 0.8491 - val_loss: 0.6202 - val_accuracy: 0.8629\n",
      "Epoch 17/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.6205 - accuracy: 0.8584 - val_loss: 0.6012 - val_accuracy: 0.8644\n",
      "Epoch 18/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.6021 - accuracy: 0.8596 - val_loss: 0.5843 - val_accuracy: 0.8644\n",
      "Epoch 19/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.5852 - accuracy: 0.8664 - val_loss: 0.5685 - val_accuracy: 0.8687\n",
      "Epoch 20/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.5697 - accuracy: 0.8670 - val_loss: 0.5535 - val_accuracy: 0.8716\n",
      "Epoch 21/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.5555 - accuracy: 0.8683 - val_loss: 0.5399 - val_accuracy: 0.8759\n",
      "Epoch 22/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.5421 - accuracy: 0.8707 - val_loss: 0.5272 - val_accuracy: 0.8759\n",
      "Epoch 23/30\n",
      "51/51 [==============================] - 0s 1000us/step - loss: 0.5296 - accuracy: 0.8751 - val_loss: 0.5151 - val_accuracy: 0.8802\n",
      "Epoch 24/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.5182 - accuracy: 0.8714 - val_loss: 0.5041 - val_accuracy: 0.8817\n",
      "Epoch 25/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.5071 - accuracy: 0.8757 - val_loss: 0.4936 - val_accuracy: 0.8874\n",
      "Epoch 26/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.4969 - accuracy: 0.8763 - val_loss: 0.4838 - val_accuracy: 0.8860\n",
      "Epoch 27/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.4872 - accuracy: 0.8794 - val_loss: 0.4743 - val_accuracy: 0.8889\n",
      "Epoch 28/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.4781 - accuracy: 0.8806 - val_loss: 0.4655 - val_accuracy: 0.8918\n",
      "Epoch 29/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.4695 - accuracy: 0.8825 - val_loss: 0.4573 - val_accuracy: 0.8918\n",
      "Epoch 30/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.4615 - accuracy: 0.8806 - val_loss: 0.4495 - val_accuracy: 0.8947\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "PMC = Sequential( )\n",
    "# Ajout de la premiere couche \"entree 􀀀> cachee\"\n",
    "# 30 neurones dans la premiere couche cachee\n",
    "# ZTrain . shape[1] : dimension du vecteur de caracteristiques en entree\n",
    "#PMC.add(layers.Dense( units=30,input_dim=X_train.shape[1] , activation='relu' ) )\n",
    "# Ajout de la seconde couche \"cachee 􀀀> cachee\"\n",
    "# 30 neurones dans la deuxieme couche cachee\n",
    "#PMC.add(layers.Dense(30, activation='relu' ) )\n",
    "# Ajout de la troisieme couche \"cachee 􀀀> sor t ie \"\n",
    "# mYTrain. shape[1] neurones dans la couche cachee =\n",
    "# nb de modalites de la variable cible\n",
    "PMC.add(layers.Dense( units=y_train.shape[1] , activation='softmax' ) )\n",
    "PMC.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print('Number of layers :',len(PMC.layers)) \n",
    "print('Training')\n",
    "#results = PMC.fit(X_train, y_train,validation_split=0.3,epochs=30)\n",
    "# fit model\n",
    "results = PMC.fit(X_train, y_train,validation_data=(X_test, y_test),epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_21 (Dense)            (None, 7)                 140       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 140\n",
      "Trainable params: 140\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "PMC.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.8837352991104126    train loss: 0.4567113220691681\n",
      "test accuracy: 0.8946608901023865     test loss: 0.4495205283164978\n"
     ]
    }
   ],
   "source": [
    "loss_train,acc_train = PMC.evaluate(X_train, y_train,verbose=0,sample_weight=None)\n",
    "print('train accuracy:', acc_train,'   train loss:', loss_train)\n",
    "loss_test,acc_test = PMC.evaluate(X_test, y_test,verbose=0,sample_weight=None)\n",
    "print('test accuracy:', acc_test,'    test loss:', loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Predicting the classes\n",
    "#PMC.predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of test classes:  693\n"
     ]
    }
   ],
   "source": [
    "predict_x=PMC.predict(X_test) \n",
    "#reverse the prediction to an integer via argmax (for multi-class predictions)\n",
    "classes_x=np.argmax(PMC.predict(X_test), axis=-1)\n",
    "print('size of test classes: ',classes_x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "class_vector =classes_x\n",
    "#print(class_vector)\n",
    " \n",
    "# Applying the function on input class vector\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_pred = to_categorical(class_vector, num_classes = 7, dtype =\"int32\")\n",
    " \n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred=enc.inverse_transform(y_pred)\n",
    "ytest=enc.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Plotting the training_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss during training\n",
    "plt.subplot(211)\n",
    "plt.title('Loss')\n",
    "plt.plot(results.history['loss'], label='train')\n",
    "plt.plot(results.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "# plot accuracy during training\n",
    "plt.subplot(212)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(results.history['accuracy'], label='train')\n",
    "plt.plot(results.history['val_accuracy'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Représentez la matrice de confusion et évaluez les performances en utilisant classification_report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[90  2  0  0  0  0  7]\n",
      " [ 3 77  1  0 10  0  8]\n",
      " [ 5  1 81  0  0  0 12]\n",
      " [ 1  0  0 97  0  0  1]\n",
      " [ 0  0  0  0 99  0  0]\n",
      " [ 0  0  0  0  0 99  0]\n",
      " [ 4  0 18  0  0  0 77]]\n"
     ]
    }
   ],
   "source": [
    "matrix = confusion_matrix(ytest, ypred)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brickface       0.87      0.91      0.89        99\n",
      "      cement       0.96      0.78      0.86        99\n",
      "     foliage       0.81      0.82      0.81        99\n",
      "       grass       1.00      0.98      0.99        99\n",
      "        path       0.91      1.00      0.95        99\n",
      "         sky       1.00      1.00      1.00        99\n",
      "      window       0.73      0.78      0.75        99\n",
      "\n",
      "    accuracy                           0.89       693\n",
      "   macro avg       0.90      0.89      0.89       693\n",
      "weighted avg       0.90      0.89      0.89       693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Testez d’autres fonction d’activation. Comparez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam',activation='softmax'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense( units=y_train.shape[1] , activation=activation) )\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-338-2baf3e35537a>:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "  model = KerasClassifier(build_fn=create_model, epochs=30, batch_size=10, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.898578 using {'activation': 'sigmoid', 'optimizer': 'RMSprop'}\n",
      "0.878169 (0.004628) with: {'activation': 'softmax', 'optimizer': 'SGD'}\n",
      "0.896104 (0.011831) with: {'activation': 'softmax', 'optimizer': 'RMSprop'}\n",
      "0.286951 (0.041550) with: {'activation': 'softmax', 'optimizer': 'Adagrad'}\n",
      "0.096475 (0.068033) with: {'activation': 'softmax', 'optimizer': 'Adadelta'}\n",
      "0.890538 (0.009214) with: {'activation': 'softmax', 'optimizer': 'Adam'}\n",
      "0.842301 (0.014919) with: {'activation': 'softmax', 'optimizer': 'Adamax'}\n",
      "0.893630 (0.005735) with: {'activation': 'softmax', 'optimizer': 'Nadam'}\n",
      "0.142857 (0.010604) with: {'activation': 'relu', 'optimizer': 'SGD'}\n",
      "0.142857 (0.010604) with: {'activation': 'relu', 'optimizer': 'RMSprop'}\n",
      "0.255411 (0.148688) with: {'activation': 'relu', 'optimizer': 'Adagrad'}\n",
      "0.157699 (0.028782) with: {'activation': 'relu', 'optimizer': 'Adadelta'}\n",
      "0.142857 (0.010604) with: {'activation': 'relu', 'optimizer': 'Adam'}\n",
      "0.174397 (0.051837) with: {'activation': 'relu', 'optimizer': 'Adamax'}\n",
      "0.142857 (0.010604) with: {'activation': 'relu', 'optimizer': 'Nadam'}\n",
      "0.882498 (0.004628) with: {'activation': 'sigmoid', 'optimizer': 'SGD'}\n",
      "0.898578 (0.010086) with: {'activation': 'sigmoid', 'optimizer': 'RMSprop'}\n",
      "0.224490 (0.084871) with: {'activation': 'sigmoid', 'optimizer': 'Adagrad'}\n",
      "0.117502 (0.067649) with: {'activation': 'sigmoid', 'optimizer': 'Adadelta'}\n",
      "0.896104 (0.006603) with: {'activation': 'sigmoid', 'optimizer': 'Adam'}\n",
      "0.849722 (0.015374) with: {'activation': 'sigmoid', 'optimizer': 'Adamax'}\n",
      "0.894249 (0.012399) with: {'activation': 'sigmoid', 'optimizer': 'Nadam'}\n",
      "0.816327 (0.010924) with: {'activation': 'softplus', 'optimizer': 'SGD'}\n",
      "0.869511 (0.006307) with: {'activation': 'softplus', 'optimizer': 'RMSprop'}\n",
      "0.327767 (0.123460) with: {'activation': 'softplus', 'optimizer': 'Adagrad'}\n",
      "0.074830 (0.034885) with: {'activation': 'softplus', 'optimizer': 'Adadelta'}\n",
      "0.858998 (0.011831) with: {'activation': 'softplus', 'optimizer': 'Adam'}\n",
      "0.790971 (0.025408) with: {'activation': 'softplus', 'optimizer': 'Adamax'}\n",
      "0.862709 (0.005462) with: {'activation': 'softplus', 'optimizer': 'Nadam'}\n",
      "0.151515 (0.038363) with: {'activation': 'softsign', 'optimizer': 'SGD'}\n",
      "0.114409 (0.047236) with: {'activation': 'softsign', 'optimizer': 'RMSprop'}\n",
      "0.111936 (0.068993) with: {'activation': 'softsign', 'optimizer': 'Adagrad'}\n",
      "0.168213 (0.031094) with: {'activation': 'softsign', 'optimizer': 'Adadelta'}\n",
      "0.168831 (0.096273) with: {'activation': 'softsign', 'optimizer': 'Adam'}\n",
      "0.096475 (0.026281) with: {'activation': 'softsign', 'optimizer': 'Adamax'}\n",
      "0.185529 (0.030183) with: {'activation': 'softsign', 'optimizer': 'Nadam'}\n",
      "0.135436 (0.060004) with: {'activation': 'tanh', 'optimizer': 'SGD'}\n",
      "0.247372 (0.096550) with: {'activation': 'tanh', 'optimizer': 'RMSprop'}\n",
      "0.153370 (0.010959) with: {'activation': 'tanh', 'optimizer': 'Adagrad'}\n",
      "0.080396 (0.026943) with: {'activation': 'tanh', 'optimizer': 'Adadelta'}\n",
      "0.173779 (0.050454) with: {'activation': 'tanh', 'optimizer': 'Adam'}\n",
      "0.144712 (0.031631) with: {'activation': 'tanh', 'optimizer': 'Adamax'}\n",
      "0.170068 (0.074771) with: {'activation': 'tanh', 'optimizer': 'Nadam'}\n",
      "0.486704 (0.076290) with: {'activation': 'selu', 'optimizer': 'SGD'}\n",
      "0.233766 (0.056457) with: {'activation': 'selu', 'optimizer': 'RMSprop'}\n",
      "0.106988 (0.050431) with: {'activation': 'selu', 'optimizer': 'Adagrad'}\n",
      "0.219542 (0.040430) with: {'activation': 'selu', 'optimizer': 'Adadelta'}\n",
      "0.186766 (0.072099) with: {'activation': 'selu', 'optimizer': 'Adam'}\n",
      "0.200989 (0.091398) with: {'activation': 'selu', 'optimizer': 'Adamax'}\n",
      "0.147186 (0.100578) with: {'activation': 'selu', 'optimizer': 'Nadam'}\n",
      "0.389610 (0.112251) with: {'activation': 'elu', 'optimizer': 'SGD'}\n",
      "0.462585 (0.018819) with: {'activation': 'elu', 'optimizer': 'RMSprop'}\n",
      "0.193568 (0.067565) with: {'activation': 'elu', 'optimizer': 'Adagrad'}\n",
      "0.141002 (0.031631) with: {'activation': 'elu', 'optimizer': 'Adadelta'}\n",
      "0.427953 (0.080690) with: {'activation': 'elu', 'optimizer': 'Adam'}\n",
      "0.194805 (0.054534) with: {'activation': 'elu', 'optimizer': 'Adamax'}\n",
      "0.380334 (0.087034) with: {'activation': 'elu', 'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=30, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "#on va tester les fonctions d'activation et les optimizateurs\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "activation = ['softmax', 'relu','sigmoid','softplus','softsign','tanh','selu','elu']\n",
    "param_grid = dict(optimizer=optimizer,activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Développez un perceptron multicouche (2 couches cachées à 30 neurones, fonctions d’activation ReLu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers : 3\n",
      "Training\n",
      "Epoch 1/30\n",
      "51/51 [==============================] - 1s 4ms/step - loss: 1.7732 - accuracy: 0.3686 - val_loss: 1.4054 - val_accuracy: 0.6421\n",
      "Epoch 2/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 1.1688 - accuracy: 0.7062 - val_loss: 0.9178 - val_accuracy: 0.8124\n",
      "Epoch 3/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.7681 - accuracy: 0.8200 - val_loss: 0.6213 - val_accuracy: 0.8600\n",
      "Epoch 4/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.5497 - accuracy: 0.8571 - val_loss: 0.4725 - val_accuracy: 0.8831\n",
      "Epoch 5/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.4252 - accuracy: 0.8887 - val_loss: 0.4022 - val_accuracy: 0.8932\n",
      "Epoch 6/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.3580 - accuracy: 0.9011 - val_loss: 0.3289 - val_accuracy: 0.9105\n",
      "Epoch 7/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.3029 - accuracy: 0.9091 - val_loss: 0.2874 - val_accuracy: 0.9120\n",
      "Epoch 8/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.2626 - accuracy: 0.9233 - val_loss: 0.2584 - val_accuracy: 0.9177\n",
      "Epoch 9/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.2364 - accuracy: 0.9258 - val_loss: 0.2310 - val_accuracy: 0.9307\n",
      "Epoch 10/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.2114 - accuracy: 0.9307 - val_loss: 0.2103 - val_accuracy: 0.9322\n",
      "Epoch 11/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.1924 - accuracy: 0.9382 - val_loss: 0.1939 - val_accuracy: 0.9365\n",
      "Epoch 12/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.1780 - accuracy: 0.9450 - val_loss: 0.1824 - val_accuracy: 0.9365\n",
      "Epoch 13/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.9406 - val_loss: 0.1741 - val_accuracy: 0.9380\n",
      "Epoch 14/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.9567 - val_loss: 0.1756 - val_accuracy: 0.9264\n",
      "Epoch 15/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.1496 - accuracy: 0.9505 - val_loss: 0.1550 - val_accuracy: 0.9408\n",
      "Epoch 16/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.1398 - accuracy: 0.9573 - val_loss: 0.1493 - val_accuracy: 0.9495\n",
      "Epoch 17/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.1328 - accuracy: 0.9598 - val_loss: 0.1423 - val_accuracy: 0.9538\n",
      "Epoch 18/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9604 - val_loss: 0.1370 - val_accuracy: 0.9495\n",
      "Epoch 19/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9617 - val_loss: 0.1350 - val_accuracy: 0.9553\n",
      "Epoch 20/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9635 - val_loss: 0.1307 - val_accuracy: 0.9509\n",
      "Epoch 21/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9660 - val_loss: 0.1310 - val_accuracy: 0.9495\n",
      "Epoch 22/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.9635 - val_loss: 0.1214 - val_accuracy: 0.9538\n",
      "Epoch 23/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9678 - val_loss: 0.1175 - val_accuracy: 0.9509\n",
      "Epoch 24/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.1010 - accuracy: 0.9660 - val_loss: 0.1170 - val_accuracy: 0.9538\n",
      "Epoch 25/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.0974 - accuracy: 0.9691 - val_loss: 0.1067 - val_accuracy: 0.9567\n",
      "Epoch 26/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9728 - val_loss: 0.1050 - val_accuracy: 0.9582\n",
      "Epoch 27/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9703 - val_loss: 0.1056 - val_accuracy: 0.9509\n",
      "Epoch 28/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9697 - val_loss: 0.1000 - val_accuracy: 0.9553\n",
      "Epoch 29/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9716 - val_loss: 0.0998 - val_accuracy: 0.9567\n",
      "Epoch 30/30\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.9697 - val_loss: 0.0962 - val_accuracy: 0.9596\n"
     ]
    }
   ],
   "source": [
    "PMC = Sequential( )\n",
    "# Ajout de la premiere couche \"entree 􀀀> cachee\"\n",
    "# 30 neurones dans la premiere couche cachee\n",
    "# ZTrain . shape[1] : dimension du vecteur de caracteristiques en entree\n",
    "PMC.add(layers.Dense( units=30,input_dim=X_train.shape[1] , activation='relu' ) )\n",
    "# Ajout de la seconde couche \"cachee 􀀀> cachee\"\n",
    "# 30 neurones dans la deuxieme couche cachee\n",
    "PMC.add(layers.Dense(30, activation='relu' ) )\n",
    "# Ajout de la troisieme couche \"cachee 􀀀> sor t ie \"\n",
    "# mYTrain. shape[1] neurones dans la couche cachee =\n",
    "# nb de modalites de la variable cible\n",
    "PMC.add(layers.Dense( units=y_train.shape[1] , activation='softmax' ) )\n",
    "PMC.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print('Number of layers :',len(PMC.layers)) \n",
    "print('Training')\n",
    "# fit model\n",
    "results = PMC.fit(X_train, y_train,validation_data=(X_test, y_test),epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 30)                600       \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 30)                930       \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 7)                 217       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,747\n",
      "Trainable params: 1,747\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "PMC.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_x=PMC.predict(X_test) \n",
    "#reverse the prediction to an integer via argmax\n",
    "classes_x=np.argmax(PMC.predict(X_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_vector =classes_x\n",
    "#print(class_vector)\n",
    " \n",
    "# Applying the function on input class vector\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_pred = to_categorical(class_vector, num_classes = 7, dtype =\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming back to one column classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred=enc.inverse_transform(y_pred)\n",
    "ytest=enc.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Représentez la matrice de confusion et évaluez les performances en utilisant classification_report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[99  0  0  0  0  0  0]\n",
      " [ 1 91  0  0  1  0  6]\n",
      " [ 0  2 91  0  0  0  6]\n",
      " [ 0  0  1 97  1  0  0]\n",
      " [ 0  0  0  0 99  0  0]\n",
      " [ 0  0  0  0  0 99  0]\n",
      " [ 0  2  8  0  0  0 89]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brickface       0.99      1.00      0.99        99\n",
      "      cement       0.96      0.92      0.94        99\n",
      "     foliage       0.91      0.92      0.91        99\n",
      "       grass       1.00      0.98      0.99        99\n",
      "        path       0.98      1.00      0.99        99\n",
      "         sky       1.00      1.00      1.00        99\n",
      "      window       0.88      0.90      0.89        99\n",
      "\n",
      "    accuracy                           0.96       693\n",
      "   macro avg       0.96      0.96      0.96       693\n",
      "weighted avg       0.96      0.96      0.96       693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix = confusion_matrix(ytest, ypred)\n",
    "print(matrix)\n",
    "print(classification_report(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the best 2-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam',activation='relu'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense( units=30,input_dim=X_train.shape[1] , activation=activation ) )\n",
    "    model.add(layers.Dense(30, activation=activation ) )\n",
    "    model.add(layers.Dense( units=y_train.shape[1] , activation='softmax') )\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-340-2baf3e35537a>:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "  model = KerasClassifier(build_fn=create_model, epochs=30, batch_size=10, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.965368 using {'activation': 'tanh', 'optimizer': 'Adam'}\n",
      "0.122449 (0.008016) with: {'activation': 'softmax', 'optimizer': 'SGD'}\n",
      "0.790353 (0.042090) with: {'activation': 'softmax', 'optimizer': 'RMSprop'}\n",
      "0.144712 (0.012943) with: {'activation': 'softmax', 'optimizer': 'Adagrad'}\n",
      "0.137291 (0.018615) with: {'activation': 'softmax', 'optimizer': 'Adadelta'}\n",
      "0.765615 (0.067649) with: {'activation': 'softmax', 'optimizer': 'Adam'}\n",
      "0.611626 (0.052788) with: {'activation': 'softmax', 'optimizer': 'Adamax'}\n",
      "0.733457 (0.081313) with: {'activation': 'softmax', 'optimizer': 'Nadam'}\n",
      "0.925170 (0.012338) with: {'activation': 'relu', 'optimizer': 'SGD'}\n",
      "0.957947 (0.002314) with: {'activation': 'relu', 'optimizer': 'RMSprop'}\n",
      "0.642548 (0.054562) with: {'activation': 'relu', 'optimizer': 'Adagrad'}\n",
      "0.163265 (0.071036) with: {'activation': 'relu', 'optimizer': 'Adadelta'}\n",
      "0.959802 (0.004870) with: {'activation': 'relu', 'optimizer': 'Adam'}\n",
      "0.936302 (0.012704) with: {'activation': 'relu', 'optimizer': 'Adamax'}\n",
      "0.963513 (0.002314) with: {'activation': 'relu', 'optimizer': 'Nadam'}\n",
      "0.654917 (0.019809) with: {'activation': 'sigmoid', 'optimizer': 'SGD'}\n",
      "0.933828 (0.002314) with: {'activation': 'sigmoid', 'optimizer': 'RMSprop'}\n",
      "0.408163 (0.106610) with: {'activation': 'sigmoid', 'optimizer': 'Adagrad'}\n",
      "0.147805 (0.008876) with: {'activation': 'sigmoid', 'optimizer': 'Adadelta'}\n",
      "0.929499 (0.004008) with: {'activation': 'sigmoid', 'optimizer': 'Adam'}\n",
      "0.871985 (0.002624) with: {'activation': 'sigmoid', 'optimizer': 'Adamax'}\n",
      "0.920841 (0.006307) with: {'activation': 'sigmoid', 'optimizer': 'Nadam'}\n",
      "0.909091 (0.003030) with: {'activation': 'softplus', 'optimizer': 'SGD'}\n",
      "0.945578 (0.012704) with: {'activation': 'softplus', 'optimizer': 'RMSprop'}\n",
      "0.632653 (0.035816) with: {'activation': 'softplus', 'optimizer': 'Adagrad'}\n",
      "0.142239 (0.013662) with: {'activation': 'softplus', 'optimizer': 'Adadelta'}\n",
      "0.952999 (0.007774) with: {'activation': 'softplus', 'optimizer': 'Adam'}\n",
      "0.909091 (0.004008) with: {'activation': 'softplus', 'optimizer': 'Adamax'}\n",
      "0.944341 (0.008434) with: {'activation': 'softplus', 'optimizer': 'Nadam'}\n",
      "0.911565 (0.009256) with: {'activation': 'softsign', 'optimizer': 'SGD'}\n",
      "0.957947 (0.004870) with: {'activation': 'softsign', 'optimizer': 'RMSprop'}\n",
      "0.734694 (0.067253) with: {'activation': 'softsign', 'optimizer': 'Adagrad'}\n",
      "0.146568 (0.054974) with: {'activation': 'softsign', 'optimizer': 'Adadelta'}\n",
      "0.958565 (0.003153) with: {'activation': 'softsign', 'optimizer': 'Adam'}\n",
      "0.930117 (0.010086) with: {'activation': 'softsign', 'optimizer': 'Adamax'}\n",
      "0.959802 (0.003153) with: {'activation': 'softsign', 'optimizer': 'Nadam'}\n",
      "0.930118 (0.006307) with: {'activation': 'tanh', 'optimizer': 'SGD'}\n",
      "0.959184 (0.002624) with: {'activation': 'tanh', 'optimizer': 'RMSprop'}\n",
      "0.753247 (0.025029) with: {'activation': 'tanh', 'optimizer': 'Adagrad'}\n",
      "0.188621 (0.076064) with: {'activation': 'tanh', 'optimizer': 'Adadelta'}\n",
      "0.965368 (0.000875) with: {'activation': 'tanh', 'optimizer': 'Adam'}\n",
      "0.939394 (0.005320) with: {'activation': 'tanh', 'optimizer': 'Adamax'}\n",
      "0.955473 (0.009460) with: {'activation': 'tanh', 'optimizer': 'Nadam'}\n",
      "0.944960 (0.004628) with: {'activation': 'selu', 'optimizer': 'SGD'}\n",
      "0.954236 (0.001749) with: {'activation': 'selu', 'optimizer': 'RMSprop'}\n",
      "0.831169 (0.020492) with: {'activation': 'selu', 'optimizer': 'Adagrad'}\n",
      "0.173160 (0.024158) with: {'activation': 'selu', 'optimizer': 'Adadelta'}\n",
      "0.954855 (0.003153) with: {'activation': 'selu', 'optimizer': 'Adam'}\n",
      "0.938157 (0.008614) with: {'activation': 'selu', 'optimizer': 'Adamax'}\n",
      "0.956710 (0.006122) with: {'activation': 'selu', 'optimizer': 'Nadam'}\n",
      "0.933828 (0.009856) with: {'activation': 'elu', 'optimizer': 'SGD'}\n",
      "0.958565 (0.006997) with: {'activation': 'elu', 'optimizer': 'RMSprop'}\n",
      "0.778602 (0.023775) with: {'activation': 'elu', 'optimizer': 'Adagrad'}\n",
      "0.189858 (0.106496) with: {'activation': 'elu', 'optimizer': 'Adadelta'}\n",
      "0.962894 (0.003030) with: {'activation': 'elu', 'optimizer': 'Adam'}\n",
      "0.931354 (0.009214) with: {'activation': 'elu', 'optimizer': 'Adamax'}\n",
      "0.964131 (0.002314) with: {'activation': 'elu', 'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=30, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "#on va tester les fonctions d'activation et les optimizateurs\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "activation = ['softmax', 'relu','sigmoid','softplus','softsign','tanh','selu','elu']\n",
    "param_grid = dict(optimizer=optimizer,activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
