
Mise en place Ecosystem IA
----------------------------
https://docs.google.com/document/d/1DRSQRxfarXCXgBY3JHio77eKVafQc13I/edit

Session 1:
https://drive.google.com/drive/folders/1pPSgsGBbD_1_WK_YaXHMSU_x7ZjXntvV


RUN PYTHON IN GITHUB VIRTUAL MACHINE - Github: test2 repo.
--------------------------------------

1) Create repo github mricardovallejo/tests2
2) Sur local directory avec application git bash
3) Init repo:  git init
3) Add files to repo :  
	git add .
	git remote add origin https://github.com/mricardovallejo/tests2.git
4) git push -u origin master

5) In order to trigger an GITHUB ACTION add a file on /.github/workflows/ecl.yaml

				name: model-supervisé RL
				on: [push]
				jobs:
				  run:
					runs-on: [ubuntu-latest]
					container: docker://dvcorg/cml-py3:latest
					steps:
					  - uses: actions/checkout@v2
					  - name: cml_run
						env:
						  repo_token: ${{ secrets.GITHUB_TOKEN }}
						run: |
						  # Your ML workflow goes here
						  pip install -r requirements.txt
						  python 01_prod_regression_ci__tutorial.py
						  # Write your CML report
						  echo "## Metriques:" >> report.md
						  cat metrics.txt >> report.md

ecl.ym has the config to docker image with image who execute my file .py 
    Function who trigger the Action (ex; push)

6) When i do modifications on .py, the PR executes the python file to obtain metrics in the PR.
All its functionality of Github. - See Actions on Github -


CML
========

https://github.com/iterative/cml#using-cml-with-dvc

https://www.youtube.com/watch?v=9BgIDqAzfuA&list=PL7WG7YrwYcnDBDuCkFbcyjnZQrdskFsBz&index=1


Continuous Machine Learning [CML] addresses the above two concerns. As indicated in their website (https://cml.dev/), 
“CML helps bring your favorite DevOps tools to machine learning”. 
CML is built as a plugin to Git Actions (which is a CI/CD tool, similar to Jenkins or TravisCI). 
The CI/CD pipeline is specified as a YAML file, and is committed along with the code to the version control system (GitHub). 
Configuring a pipeline using CML is very simple. 


 Continuous Machine Learning (CML) is an open-source library for implementing continuous integration & delivery (CI/CD) in machine learning projects. 
 Use it to automate parts of your development workflow, including model training and evaluation, comparing ML experiments across your project history, and monitoring changing datasets.
 
 On every pull request, CML helps you automatically train and evaluate models, then generates a visual report with results and metrics. 
 
 
Ecosysteme ML
Déploiement de modele
Gestion de version- données et modèle
====================================
https://github.com/EthicalML/awesome-production-machine-learning


Gouvernance during le dev du Modele
	Comment faire pour faire le suivi des activités réalisées durant le développement du modele 
	Format de livraison du modele afin de reproduire les resultats sur n’importe quelle plateforme 
	Enregistrer et evaluer les experiences précédentes: code, data, config, métriques et autres resultats
	Reproduire le processus de developpement
	
	Ex: mlflow

Déploiement et Monitoring

	Pour la phase de monitoring, on devra identifier:
	Les métriques à utiliser pour le suivi de monitoring
	Combien de features sont présentes dans le modèle en production. Combien sont en train d’etre suivies?
	Comment est-on en train de suivre les features numériques et catégorielles?
	Comment a-t-on sélectionné les seuils de suivi de ces features?
	Si un seuil est dépassé, quelles sont les étapes suivantes à faire sur la feature?
	Est-ce que le suivi est ponctuel ou permanent?
	Définir les circonstances qui vont faire en sorte que le modèle doit faire l’objet d’un nouvel apprentissage 

Contrôle de version pour ML
	Un projet IA fait intervenir plusieurs compétences
	Data Engineers
	ML Engineers
	Business Analysts
	Spécialiste pour le déploiement du modele
	DevOps 
	Les gestionnaires pour le coté exploitation
	Structure du projet peut etre critique:
	Structure du projet en tant que repertoire contenant les artefacts
	Data Versioning
	Gestion du Packaging & Environnement
	Résultats de l’analyse en terme d’exploitation
	Interprétation des résultats ML 

Structure du projet
	Il est important de trouver un moyen de gérer les différents artefacts.
	Structure du repertoire doit etre
	Claire
	Facile à organiser
	Exprime l’organisation du projet
	Permettre de réduire les bugs
	Permettre de protéger la propriété intellectuelle (modele) 
	Faciliter la reproduction et la repetabilité du processus 
	Permettre l’extraction des résultats précédents
	Logs
	Metriques
	Etc. 

Outils disponibles
	mlflow
	Cookiecutter Data Science (https://drivendata.github.io/cookiecutter-data-science/)
	Pachyderm 
	Modeldb

Versioning des données
	Data is immutable (no overwrite)
	Sauvegarder les artefacts intermediaires
	Reproducible & Repetable (inputs & outputs doivent etre suivis)
	Probleme: Data files sont en générales assez larges et les GCS ont de la difficulté à les gérer

	Outils
		pachyderm
		AWS S3
		Git Large File Storage
		git-annex
		dat
		Network File Server (Ceph, FreeNAS, ZFS)
		dvc

DVC
	But: DVC is built to make ML models shareable and reproducible. It is designed to handle large files, data sets, machine learning models, and metrics as well as code.
	Site: dvc.org
	Bénéfices
	Version control pour modeles et data
	Définir workflow pour deploiement & collaboration

Exemple de Data Versioning
	“DVC allows storing and versioning source data files, ML models, intermediate results with Git, without checking the file contents into Git”

Gestion Environnement & Package du modele

	Faciliter la reproduction de résultats 
	Faciliter la reproduction de processus 
	Gérer la livraison du modele pour execution sur n’importe quelle plateforme  


Architecture 
Développement-Déploiement ML
=================================================
Intégration continue: bénéfices

	Processus d’intégration souple
	Test de régression automatique
	Release du produit régulière
	Test fonctionnel assez tôt dans le cycle
	Résolution des bugs rapide
	Visibilité du projet 
	
Pour démarrer en IC
	Processus de build automatique  
	Tests automatiques  
	Référentiel pour le code  / Gestionnaire de code source
	Serveur d’intégration continue  

Intégration continue en quelques mots
	Suivi des problèmes d’intégration
	Publication automatique des artifacts  
	Suivi du processus de build
	Suivi et rapport sur la qualité du code

Cycle de développement-déploiement ML
	Collecte-Acquisition
	Exploration
	Pré-traitement
	Modele
	Mise en production
	Utilisation

Comparaison avec cycle du logiciel
	Les modèles qu’on doit déployer doivent être:
	Reproductible
	Testable
	Peuvent passer un audit
	Surtout: capable d’etre continuellement amélioré

Problèmes avec cette approche ML
	Il n’y a pas de solutions standard
	Chaque projet a ses propres contraintes
	Processus est difficile à tester
	Chaque étape demande une certaine expertise
	Collecte: Data engineer
	Exploration: Stats, Viz
	Etc.
	Amélioration d’une solution est encore plus difficile

Sources de changement
	Données
		Modèle de données
		Sources de données peuvent varier dans le temos
		Volume/Vitesse
		etc
	Modèle
		État de l’art change très vite
		Beaucoup de recherche dans le domaine
		Performance bonne aujourd’hui n’est pas garantie pour demain!
	Code et application pour consommer le modele
		Nouvelle demande de clients usagers
		Bugs
		Dépendances qui peuvent changer dans le temps
		Etc.

Mise en production continue
		
	La mise en production en continue (continuous delivery) permet:
	De prendre les changements tels que les nouvelles fonctionnalités
	Bugs, 
	Nouveau data
	Etc.
	Et les met en production de façon efficace et rapide 

	Définir et créer un processus répétable et fiable pour la mise en production du modèle
	Permettre d’automatiser la presque totalité du processus
	Permettre de faire un suivi de changement (change control)
	Permettre de garder tous les artefacts dans un gestionnaire d’artefacts
	Suivre et gérer le cycle du modèle de manière coninue

Mise en production | Après !
	Comment procéder à un nouveau apprentissage le plus fréquemment possible?
	Comment éviter des problèmes de clash de version de déploiement?
	Comment redéployer le modèle mis à jour?
	Comment être sur que le processus de développement est toujours d’acualité?
	Comment mesure la performance de notre modèle?

Suivi du développement
	Il ne suffit pas de déployer, il faut aussi un suivi du processus de développement
	Quelles sont les hypotheses qui sont en train d’etre explorées?
	Quelles techniques de pré-traitement a t-on essayées?
	Combien de temps prend chaque processus pour s’executer?
	Quels sont les parametres et hyperparametres utilisées?
	Quels sont les métriques utilizes?
	Etc.

Amélioration continue
	Une fois en production, on doit capturer des métriques de production afin d’améliorer nos modèles
	Suivre comment le modèle est utilisé
	Voir quel type de données on utilise sur le modèle
	Évaluer la sortie du modèle pour voir son écart par rapport à la normale
	Essayer de détecter s’il y’a un overfit
	Essayer de voir si le modèle n’a pas de biais
	Etc


